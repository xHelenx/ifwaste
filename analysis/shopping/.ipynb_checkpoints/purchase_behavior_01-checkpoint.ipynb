{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99340691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"/home/haasehelen/haasehelen/ifwaste/output\")\n",
    "EXCLUDE_COLUMNS = ['n_quickcook', 'n_cook', 'n_attempted_cook', 'n_leftovers', 'n_shop',\n",
    "                   'n_quickshop', 'n_attempted_shop']\n",
    "\n",
    "scenario_folders = [\n",
    "    \"02_scenario0_no_promotions\", \n",
    "    \"02_scenario1_bogos_only\", \n",
    "    \"02_scenario2_sales_only\", \n",
    "    \"02_scenario3_both\"\n",
    "]\n",
    "\n",
    "scenario_data = {}\n",
    "\n",
    "for scenario in scenario_folders:\n",
    "    scenario_path = base_path / scenario\n",
    "    combined_rows = []\n",
    "    combined_bought = []  # Collect all bought DataFrames per run\n",
    "    print(scenario_path)\n",
    "    \n",
    "    for run_folder in scenario_path.glob(\"run_*\"):\n",
    "        run_id = run_folder.name.split(\"_\")[1]\n",
    "        \n",
    "        config_file = run_folder / \"log_hh_config.csv\"\n",
    "        output_file = run_folder / \"aggregated_outputs.csv\"\n",
    "        bought_file = run_folder / \"log_bought.csv\"\n",
    "\n",
    "        if not config_file.exists() or not output_file.exists() or not bought_file.exists():\n",
    "            print(f\"Skipping {run_folder} due to missing files.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df_config = dd.read_csv(config_file)\n",
    "            df_config[\"household\"] = df_config[\"household\"].astype(int)\n",
    "            df_output = dd.read_csv(output_file)\n",
    "            df_output[\"household\"] = df_output[\"household\"].astype(int)\n",
    "            df_bought = dd.read_csv(bought_file)\n",
    "            df_bought[\"household\"] = df_bought[\"household\"].astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading files in {run_folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "        merged_df = dd.merge(df_config, df_output, on=\"household\", how=\"inner\")\n",
    "\n",
    "        # Add unique key â€” use vectorized string concatenation via assign + map_partitions\n",
    "        merged_df = merged_df.assign(run_household_key=merged_df[\"household\"].map_partitions(\n",
    "            lambda s: \"run_\" + run_id + \"_\" + s.astype(str)\n",
    "        ))\n",
    "        merged_df = merged_df.drop(columns=EXCLUDE_COLUMNS, errors='ignore')\n",
    "        combined_rows.append(merged_df)\n",
    "\n",
    "        # For bought, add run_household_key similarly\n",
    "        df_bought = df_bought.assign(run_household_key=df_bought[\"household\"].map_partitions(\n",
    "            lambda s: \"run_\" + run_id + \"_\" + s.astype(str)\n",
    "        ))\n",
    "        combined_bought.append(df_bought)\n",
    "    \n",
    "    if combined_rows:\n",
    "        scenario_data[scenario] = {}\n",
    "        scenario_data[scenario][\"general\"] = dd.concat(combined_rows, interleave_partitions=True)\n",
    "        scenario_data[scenario][\"bought\"] = dd.concat(combined_bought, interleave_partitions=True)\n",
    "        print(f\"{scenario}: Combined {len(combined_rows)} runs.\")\n",
    "    else:\n",
    "        print(f\"{scenario}: No valid data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f40c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifwaste-env",
   "language": "python",
   "name": "ifwaste-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
