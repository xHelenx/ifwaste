{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to analyze IFWASTE-Simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"/blue/carpena/haasehelen/ifwaste/data/\"\n",
    "#CONFIG_PATH = \"/blue/carpena/haasehelen/ifwaste/model/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"E:/UF/ifwaste/data/\"\n",
    "CONFIG_PATH = \"E:/UF/ifwaste/model/config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary parameters from config file\n",
    "- weights per serving of each food category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FGMEAT_KG = FGDAIRY_KG = FGBAKED_KG = FGVEGETABLE_KG = FGDRYFOOD_KG = FGSNACKS_KG = FGSTOREPREPARED_KG = None\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = json.load(f)\n",
    "    FGMEAT_KG = config[\"Food\"][\"FGMeat\"][\"kg_per_serving\"]\n",
    "    FGDAIRY_KG = config[\"Food\"][\"FGDairy\"][\"kg_per_serving\"]\n",
    "    FGBAKED_KG = config[\"Food\"][\"FGBaked\"][\"kg_per_serving\"]\n",
    "    FGVEGETABLE_KG = config[\"Food\"][\"FGVegetable\"][\"kg_per_serving\"]\n",
    "    FGDRYFOOD_KG = config[\"Food\"][\"FGDryFood\"][\"kg_per_serving\"]\n",
    "    FGSNACKS_KG = config[\"Food\"][\"FGSnacks\"][\"kg_per_serving\"]\n",
    "    FGSTOREPREPARED_KG = config[\"Food\"][\"FGStorePrepared\"][\"kg_per_serving\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df = pd.DataFrame({\n",
    "    'Type': ['Meat & Fish', 'Dairy & Eggs', 'Fruits & Vegetables', 'Dry Foods', 'Baked Goods' ,\n",
    "    'Snacks, Condiments, Oils & Other', 'Store-Prepared Items'],\n",
    "    'Servings_to_Kg': [FGMEAT_KG, FGDAIRY_KG, FGVEGETABLE_KG, FGDRYFOOD_KG, FGBAKED_KG, FGSNACKS_KG, FGSTOREPREPARED_KG],\n",
    "    \"Color\": [\"#116A65\", \"#00a0e1\", \"#466eb4\", \"#e6a532\", \"#d7642c\",\"#73B55B\", \"#D82E5E\"]\n",
    "})\n",
    "\n",
    "status_colors = {\n",
    "            \"Inedible Parts\": \"#26547C\",\n",
    "            \"Plate Waste\": \"#FFD166\",\n",
    "            \"Spoiled Food\": \"#EF476F\"\n",
    "        }\n",
    "    \n",
    "#color_mapping = dict(zip(lookup_df['Type'], lookup_df['Color']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> dict[str, dict[str, dd.DataFrame]]:\n",
    "    # Dictionary to store DataFrames for each run\n",
    "    runs_data = {}\n",
    "\n",
    "    # List of the main folders to process\n",
    "    main_folders = [f for f in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, f))]\n",
    "    # Iterate over each main folder\n",
    "    for main_folder in main_folders:\n",
    "        folder_path = os.path.join(PATH, main_folder)  # type: ignore\n",
    "        \n",
    "        # Iterate over each run folder inside the main folder\n",
    "        for run_folder in os.listdir(folder_path):\n",
    "            run_path = os.path.join(folder_path, run_folder)\n",
    "            if os.path.isdir(run_path):\n",
    "                run_id = run_folder  # Use the folder name as the run ID (e.g., 'run_0')\n",
    "                # Initialize a dictionary to store DataFrames for each log file in this run\n",
    "                run_logs = {}\n",
    "                # Get all CSV files in the run folder\n",
    "                file_names = [\n",
    "                    f for f in os.listdir(run_path)\n",
    "                    if os.path.isfile(os.path.join(run_path, f)) and f.endswith('.csv')\n",
    "                ]\n",
    "                \n",
    "                # Read each CSV file into a Dask DataFrame\n",
    "                for file in file_names:\n",
    "                    file_path = os.path.normpath(os.path.join(run_path, file))\n",
    "                    if os.path.exists(file_path):\n",
    "                        log_name = file[:-4]  # Remove '.csv' to get the log name\n",
    "                        run_logs[log_name] = dd.read_csv(file_path,assume_missing=True) #assume all values as floats\n",
    "                \n",
    "                # Add the logs dictionary to the runs_data under the current run ID\n",
    "                runs_data[run_id] = run_logs\n",
    "\n",
    "    return runs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_merged_data() -> dict[str, dd.DataFrame]:\n",
    "    file_names = ['log_bought', 'log_eaten', 'log_wasted', 'log_hh_config', \"log_still_have\",\n",
    "                  \"log_hh_daily\", \"log_sim_config\", \"log_store_daily\", \"log_wasted\"]\n",
    "    \n",
    "    # Dictionary to store merged DataFrames for each CSV type across all runs\n",
    "    data_dict = {}\n",
    "\n",
    "    # List of the main folders to process\n",
    "    main_folders = [f for f in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, f))]\n",
    "    \n",
    "    # Iterate over each main folder\n",
    "    for main_folder in main_folders:\n",
    "        folder_path = os.path.join(PATH, main_folder)\n",
    "        \n",
    "        # Iterate over each run folder inside the main folder\n",
    "        for run_folder in os.listdir(folder_path):\n",
    "            run_path = os.path.join(folder_path, run_folder)\n",
    "            if os.path.isdir(run_path):\n",
    "                run_id = run_folder.split('_')[-1]  # Extract the run ID from the folder name\n",
    "\n",
    "                # Iterate over each CSV type\n",
    "                for csv_type in file_names:\n",
    "                    file_path = os.path.join(run_path, f'{csv_type}.csv')\n",
    "                    if os.path.exists(file_path):\n",
    "                        # Read the CSV file into a Dask DataFrame\n",
    "                        df = dd.read_csv(file_path, assume_missing=True)\n",
    "                        \n",
    "                        # Modify the 'household' column if it exists\n",
    "                        if 'household' in df.columns:\n",
    "                            df['household'] = run_id + \"_\" + df['household'].astype(int).astype(str)\n",
    "                        if \"Unnamed: 0\" in df.columns: \n",
    "                            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "                        \n",
    "                        # Rename columns that have \"Day\" in their name to just \"Day\"\n",
    "                        for item in df.columns: \n",
    "                            if \"Day\" in item: \n",
    "                                df = df.rename(columns={item: \"Day\"})\n",
    "                        \n",
    "                        # Append the DataFrame to the corresponding entry in data_dict\n",
    "                        if csv_type not in data_dict:\n",
    "                            data_dict[csv_type] = df\n",
    "                        else:\n",
    "                            # Concatenate with the existing DataFrame\n",
    "                            data_dict[csv_type] = dd.concat([data_dict[csv_type], df], axis=0)\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = load_merged_data()\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>household</th>\n",
       "      <th>day</th>\n",
       "      <th>type</th>\n",
       "      <th>servings</th>\n",
       "      <th>days_till_expiry</th>\n",
       "      <th>price_per_serving</th>\n",
       "      <th>sale_type</th>\n",
       "      <th>discount_effect</th>\n",
       "      <th>amount</th>\n",
       "      <th>sale_timer</th>\n",
       "      <th>store</th>\n",
       "      <th>product_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FGDAIRY</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Premium_retailer at (4, 6)</td>\n",
       "      <td>FGDAIRY61.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FGDRYFOOD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Premium_retailer at (4, 6)</td>\n",
       "      <td>FGDRYFOOD60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FGSNACKS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Premium_retailer at (4, 6)</td>\n",
       "      <td>FGSNACKS32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FGVEGETABLE</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Premium_retailer at (4, 6)</td>\n",
       "      <td>FGVEGETABLE61.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FGBAKED</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Premium_retailer at (4, 6)</td>\n",
       "      <td>FGBAKED101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>5_40</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FGSTOREPREPARED</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>EnumSales.SEASONAL</td>\n",
       "      <td>EnumDiscountEffect.BOGO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Discount_retailer at (6, 3)</td>\n",
       "      <td>FGSTOREPREPARED121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>5_10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FGDRYFOOD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>EnumSales.SEASONAL</td>\n",
       "      <td>EnumDiscountEffect.DISCOUNT30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Convenience_store at (3, 5)</td>\n",
       "      <td>FGDRYFOOD60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>5_10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FGSTOREPREPARED</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>EnumSales.EXPIRING</td>\n",
       "      <td>EnumDiscountEffect.DISCOUNT60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Convenience_store at (3, 5)</td>\n",
       "      <td>FGSTOREPREPARED61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>5_10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FGSTOREPREPARED</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>EnumSales.NONE</td>\n",
       "      <td>EnumDiscountEffect.NONE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Convenience_store at (3, 5)</td>\n",
       "      <td>FGSTOREPREPARED121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>5_21</td>\n",
       "      <td>9.0</td>\n",
       "      <td>FGDAIRY</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>EnumSales.SEASONAL</td>\n",
       "      <td>EnumDiscountEffect.DISCOUNT20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Premium_retailer at (7, 5)</td>\n",
       "      <td>FGDAIRY61.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49875 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     household  day             type  servings  days_till_expiry  \\\n",
       "0         0_76  0.0          FGDAIRY       6.0              11.0   \n",
       "1         0_76  0.0        FGDRYFOOD       6.0              21.0   \n",
       "2         0_76  0.0         FGSNACKS       3.0              20.0   \n",
       "3         0_76  0.0      FGVEGETABLE       6.0              14.0   \n",
       "4         0_33  0.0          FGBAKED      10.0              11.0   \n",
       "...        ...  ...              ...       ...               ...   \n",
       "1598      5_40  9.0  FGSTOREPREPARED      24.0               4.0   \n",
       "1599      5_10  9.0        FGDRYFOOD       6.0              31.0   \n",
       "1600      5_10  9.0  FGSTOREPREPARED       6.0               1.0   \n",
       "1601      5_10  9.0  FGSTOREPREPARED      12.0               5.0   \n",
       "1602      5_21  9.0          FGDAIRY       6.0              21.0   \n",
       "\n",
       "      price_per_serving           sale_type                discount_effect  \\\n",
       "0              1.800000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "1              0.500000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "2              2.000000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "3              1.300000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "4              1.000000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "...                 ...                 ...                            ...   \n",
       "1598           0.250000  EnumSales.SEASONAL        EnumDiscountEffect.BOGO   \n",
       "1599           0.350000  EnumSales.SEASONAL  EnumDiscountEffect.DISCOUNT30   \n",
       "1600           0.666667  EnumSales.EXPIRING  EnumDiscountEffect.DISCOUNT60   \n",
       "1601           1.000000      EnumSales.NONE        EnumDiscountEffect.NONE   \n",
       "1602           1.440000  EnumSales.SEASONAL  EnumDiscountEffect.DISCOUNT20   \n",
       "\n",
       "      amount  sale_timer                        store            product_ID  \n",
       "0       11.0      1000.0   Premium_retailer at (4, 6)           FGDAIRY61.8  \n",
       "1       25.0      1000.0   Premium_retailer at (4, 6)         FGDRYFOOD60.5  \n",
       "2       12.0      1000.0   Premium_retailer at (4, 6)          FGSNACKS32.0  \n",
       "3        3.0      1000.0   Premium_retailer at (4, 6)       FGVEGETABLE61.3  \n",
       "4        2.0      1000.0   Premium_retailer at (4, 6)          FGBAKED101.0  \n",
       "...      ...         ...                          ...                   ...  \n",
       "1598     1.0         5.0  Discount_retailer at (6, 3)  FGSTOREPREPARED121.0  \n",
       "1599     1.0         4.0  Convenience_store at (3, 5)         FGDRYFOOD60.5  \n",
       "1600     1.0      1000.0  Convenience_store at (3, 5)   FGSTOREPREPARED61.0  \n",
       "1601     1.0      1000.0  Convenience_store at (3, 5)  FGSTOREPREPARED121.0  \n",
       "1602     1.0         5.0   Premium_retailer at (7, 5)           FGDAIRY61.8  \n",
       "\n",
       "[49875 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = merged_data[\"log_bought\"].compute()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of houses\n",
    "N_HH = data[\"run_0\"][\"log_hh_config\"][\"household\"].max().compute() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_config = pd.DataFrame(data[\"run_0\"][\"log_sim_config\"])\n",
    "DAYS = sim_config[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_KEYS = data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks: \n",
    "1. Biomass check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group:  run_0\n",
      "bought total servings:\t\t 33431\n",
      "eaten total servings:\t\t 27350\n",
      "wasted total servings:\t\t 10880\n",
      "left last day in servings:\t 4888\n",
      "bought - consumed:\t\t -9687.902123476357\n",
      "------------\n",
      "group:  run_1\n",
      "bought total servings:\t\t 32694\n",
      "eaten total servings:\t\t 26295\n",
      "wasted total servings:\t\t 9444\n",
      "left last day in servings:\t 5807\n",
      "bought - consumed:\t\t -8853.165561652131\n",
      "------------\n",
      "group:  run_2\n",
      "bought total servings:\t\t 34323\n",
      "eaten total servings:\t\t 27109\n",
      "wasted total servings:\t\t 7751\n",
      "left last day in servings:\t 6714\n",
      "bought - consumed:\t\t -7252.448590026484\n",
      "------------\n",
      "group:  run_3\n",
      "bought total servings:\t\t 33622\n",
      "eaten total servings:\t\t 27932\n",
      "wasted total servings:\t\t 8807\n",
      "left last day in servings:\t 6278\n",
      "bought - consumed:\t\t -9396.487380695005\n",
      "------------\n",
      "group:  run_4\n",
      "bought total servings:\t\t 35437\n",
      "eaten total servings:\t\t 27929\n",
      "wasted total servings:\t\t 8899\n",
      "left last day in servings:\t 6732\n",
      "bought - consumed:\t\t -8125.054325497367\n",
      "------------\n",
      "group:  run_5\n",
      "bought total servings:\t\t 33142\n",
      "eaten total servings:\t\t 26680\n",
      "wasted total servings:\t\t 8699\n",
      "left last day in servings:\t 5662\n",
      "bought - consumed:\t\t -7900.590155942256\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for key in EXPERIMENT_KEYS: \n",
    "    bought = data[key][\"log_bought\"][\"servings\"] * data[key][\"log_bought\"][\"amount\"]\n",
    "    bought = bought.sum().compute()\n",
    "    \n",
    "    eaten = data[key][\"log_eaten\"][\"servings\"].sum().compute()\n",
    "    wasted = data[key][\"log_wasted\"][\"servings\"].sum().compute()\n",
    "    left = data[key][\"log_still_have\"][\"servings\"].sum().compute()\n",
    "    \n",
    "    print(\"group: \", key)\n",
    "    print(\"bought total servings:\\t\\t\", int(bought))\n",
    "    print(\"eaten total servings:\\t\\t\", int(eaten))\n",
    "    print(\"wasted total servings:\\t\\t\", int(wasted))    \n",
    "    print(\"left last day in servings:\\t\", int(left))\n",
    "    \n",
    "    diff = bought - (eaten + wasted + left)\n",
    "\n",
    "    print(\"bought - consumed:\\t\\t\", diff)\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33704.2775768385"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what all houes of run 0 should eat in days\n",
    "should_eat = data[\"run_0\"][\"log_hh_config\"][\"required_servings\"].sum().compute() * DAYS\n",
    "should_eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3234.1433984014416"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much servings are missing over all hh of run 0 over days\n",
    "missing_eat = data[\"run_0\"][\"log_hh_daily\"][\"servings\"].sum().compute()\n",
    "missing_eat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['log_bought', 'log_eaten', 'log_wasted', 'log_hh_config', 'log_still_have', 'log_hh_daily', 'log_sim_config', 'log_store_daily'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30470.134178437063"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eaten = should_eat - missing_eat\n",
    "eaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data for per capita and per household baseline values\n",
    "have a second datastructure in place that has all the hh merged together for some analysis like average waste per household. \n",
    "\n",
    "data <- holds data organized by run\n",
    "\n",
    "data_merged <- holds data organized as 1 big simulation, hh unique through HH+RUN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(dask_df, per_capita):\n",
    "    # Extract Dask DataFrames\n",
    "    print(dask_df.keys())\n",
    "    config_df = dask_df[\"log_hh_config\"].compute()\n",
    "    wasted = dask_df[\"log_wasted\"]\n",
    "    \n",
    "    # Merge data with configuration to get household information\n",
    "    merged_df = dd.merge(wasted, config_df[['household', 'adults', 'children']], on='household', how='inner')\n",
    "    \n",
    "    # Calculate the total number of people\n",
    "    total_people = 1\n",
    "    if per_capita:\n",
    "        total_people = config_df['adults'][0].values[0] + config_df['children'][0].values[0]\n",
    "    # Group by day and household, then sum waste and normalize if per capita\n",
    "    df_grouped = merged_df.groupby(by=['day', 'household'])['servings'].sum().compute()\n",
    "    df_grouped = df_grouped / total_people\n",
    "\n",
    "    # Convert to a DataFrame and reset index\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "\n",
    "    # Create a DataFrame with all days and houses\n",
    "    all_houses = merged_df['household'].unique().compute()\n",
    "    print(all_houses)\n",
    "    all_combinations = pd.MultiIndex.from_product([range(DAYS), all_houses], names=['day', 'household'])\n",
    "    \n",
    "    # Reindex to include all days and houses, filling missing values with 0\n",
    "    df_complete = df_grouped.set_index(['day', 'household']).reindex(all_combinations, fill_value=0).reset_index()\n",
    "    print(df_complete.head())\n",
    "    print(df_complete[\"servings\"].std())\n",
    "    print(df_complete[\"servings\"].mean())\n",
    "    # Convert back to a Dask DataFrame\n",
    "    df_complete = dd.from_pandas(df_complete, npartitions=1)\n",
    "    \n",
    "\n",
    "    return df_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['log_bought', 'log_eaten', 'log_wasted', 'log_hh_config', 'log_still_have', 'log_hh_daily', 'log_sim_config', 'log_store_daily'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helen\\miniconda3\\envs\\ifwaste-env\\Lib\\site-packages\\dask\\dataframe\\multi.py:520: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+----------------------------+------------+-------------+\n",
      "| Merge columns              | left dtype | right dtype |\n",
      "+----------------------------+------------+-------------+\n",
      "| ('household', 'household') | object     | string      |\n",
      "+----------------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0_5\n",
      "1     0_25\n",
      "2     0_43\n",
      "3     0_11\n",
      "4      0_3\n",
      "      ... \n",
      "25    1_36\n",
      "26    2_23\n",
      "27    2_27\n",
      "28    2_48\n",
      "29    2_19\n",
      "Name: household, Length: 150, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prep_0k \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m prep_0k\u001b[38;5;241m.\u001b[39mhead\n",
      "Cell \u001b[1;32mIn[16], line 28\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(dask_df, per_capita)\u001b[0m\n\u001b[0;32m     26\u001b[0m all_houses \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousehold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_houses)\n\u001b[1;32m---> 28\u001b[0m all_combinations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mMultiIndex\u001b[38;5;241m.\u001b[39mfrom_product([\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDAYS\u001b[49m\u001b[43m)\u001b[49m, all_houses], names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousehold\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Reindex to include all days and houses, filling missing values with 0\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df_complete \u001b[38;5;241m=\u001b[39m df_grouped\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousehold\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mreindex(all_combinations, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "prep_0k = preprocess_data(merged_data,True)\n",
    "prep_0k.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifwaste-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
